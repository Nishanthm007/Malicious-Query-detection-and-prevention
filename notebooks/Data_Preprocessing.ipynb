{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77cf65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TASK 4: DATA PREPROCESSING\n",
      " Building EDA-driven preprocessing pipeline...\n",
      "LOADING SQL INJECTION DATASET\n",
      "Auto-detected dataset: clean_sql_dataset.csv\n",
      " Loading from: c:\\Users\\nisha\\OneDrive\\Desktop\\Major-Project\\Malicious-Query-detection-and-prevention\\data\\raw\\clean_sql_dataset.csv\n",
      " Successfully loaded 148,326 records\n",
      " Columns: ['query', 'label']\n",
      " Dataset loaded: (148326, 2)\n",
      " Columns: ['query', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" TASK 4: DATA PREPROCESSING\")\n",
    "\n",
    "print(\" Building EDA-driven preprocessing pipeline...\")\n",
    "\n",
    "# Setup paths\n",
    "project_root = os.path.abspath('..')\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Load data using our established pipeline\n",
    "from preprocessing.data_loader import DatasetLoader\n",
    "config_path = os.path.join(project_root, 'config.json')\n",
    "loader = DatasetLoader(config_path=config_path)\n",
    "df = loader.load_sql_injection_dataset()\n",
    "\n",
    "print(f\" Dataset loaded: {df.shape}\")\n",
    "print(f\" Columns: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd413d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EDA-DRIVEN DATA CLEANING:\n",
      " Starting with: 143,210 records\n",
      "\n",
      "1️ Handling missing values...\n",
      "    No missing values in critical columns\n",
      "\n",
      " Removing complete duplicates...\n",
      "    Removed 4 complete duplicate records\n",
      "\n",
      " Removing duplicate queries...\n",
      "    Removed 10,775 duplicate queries\n",
      "\n",
      " EDA Recommendation #7: Filtering short queries (<10 chars)...\n",
      "   Short queries found: 34\n",
      "   Examples to be removed:\n",
      "     1. 'or true--' (length: 9, class: Malicious)\n",
      "     2. 'or 3 = 3' (length: 8, class: Malicious)\n",
      "     3. 'or '' = '' (length: 9, class: Malicious)\n",
      "    Removed 34 queries shorter than 10 characters\n",
      "\n",
      " Cleaning edge cases...\n",
      "    No edge cases found\n",
      "\n",
      " CLEANING SUMMARY:\n",
      "   Original dataset: 143,210 records\n",
      "   Final dataset: 132,397 records\n",
      "   Total removed: 10,813 records (7.6%)\n",
      "   Data retention: 92.4%\n",
      "\n",
      " Cleaning log:\n",
      "   1. Complete duplicates removed: 4\n",
      "   2. Query duplicates removed: 10,775\n",
      "   3. Short queries removed: 34\n",
      "\n",
      " Class Distribution (After Cleaning):\n",
      "   • 0 (Normal): 65,656 (49.6%)\n",
      "   • 1 (Malicious): 66,741 (50.4%)\n",
      "   Balance ratio after cleaning: 1.02:1\n",
      "    Excellent class balance maintained!\n",
      "\n",
      " Data cleaning completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#Apply EDA-driven data cleaning\n",
    "print(\"\\n EDA-DRIVEN DATA CLEANING:\")\n",
    "\n",
    "original_count = len(df)\n",
    "print(f\" Starting with: {len(df):,} records\")\n",
    "cleaning_log = []\n",
    "\n",
    "# Cleaning Operation 1: Handle missing values\n",
    "print(f\"\\n1️ Handling missing values...\")\n",
    "if 'query' in df.columns and 'label' in df.columns:\n",
    "    before_missing = len(df)\n",
    "    df = df.dropna(subset=['query', 'label']).copy()\n",
    "    missing_removed = before_missing - len(df)\n",
    "    \n",
    "    if missing_removed > 0:\n",
    "        print(f\"    Removed {missing_removed:,} records with missing query/label\")\n",
    "        cleaning_log.append(f\"Missing values removed: {missing_removed:,}\")\n",
    "    else:\n",
    "        print(f\"    No missing values in critical columns\")\n",
    "\n",
    "# Cleaning Operation 2: Remove complete duplicates\n",
    "print(f\"\\n Removing complete duplicates...\")\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "complete_dups_removed = before_dedup - len(df)\n",
    "\n",
    "if complete_dups_removed > 0:\n",
    "    print(f\"    Removed {complete_dups_removed:,} complete duplicate records\")\n",
    "    cleaning_log.append(f\"Complete duplicates removed: {complete_dups_removed:,}\")\n",
    "else:\n",
    "    print(f\"    No complete duplicates found\")\n",
    "\n",
    "# Cleaning Operation 3: Remove query duplicates (keep first occurrence)\n",
    "print(f\"\\n Removing duplicate queries...\")\n",
    "before_query_dedup = len(df)\n",
    "df = df.drop_duplicates(subset=['query'], keep='first').reset_index(drop=True)\n",
    "query_dups_removed = before_query_dedup - len(df)\n",
    "\n",
    "if query_dups_removed > 0:\n",
    "    print(f\"    Removed {query_dups_removed:,} duplicate queries\")\n",
    "    cleaning_log.append(f\"Query duplicates removed: {query_dups_removed:,}\")\n",
    "else:\n",
    "    print(f\"    No duplicate queries found\")\n",
    "\n",
    "# Cleaning Operation 4: EDA Recommendation #7 - Filter very short queries\n",
    "print(f\"\\n EDA Recommendation #7: Filtering short queries (<10 chars)...\")\n",
    "if 'query' in df.columns:\n",
    "    df['query_length'] = df['query'].astype(str).str.len()\n",
    "    before_filter = len(df)\n",
    "    \n",
    "    short_queries = df[df['query_length'] < 10]\n",
    "    print(f\"   Short queries found: {len(short_queries):,}\")\n",
    "    \n",
    "    # Show examples before removal\n",
    "    if len(short_queries) > 0:\n",
    "        print(f\"   Examples to be removed:\")\n",
    "        for i, (idx, row) in enumerate(short_queries.head(3).iterrows()):\n",
    "            query = str(row['query'])\n",
    "            label_name = \"Normal\" if row['label'] == 0 else \"Malicious\"\n",
    "            print(f\"     {i+1}. '{query}' (length: {len(query)}, class: {label_name})\")\n",
    "    \n",
    "    # Remove short queries\n",
    "    df = df[df['query_length'] >= 10].copy()\n",
    "    short_removed = before_filter - len(df)\n",
    "    \n",
    "    if short_removed > 0:\n",
    "        print(f\"    Removed {short_removed:,} queries shorter than 10 characters\")\n",
    "        cleaning_log.append(f\"Short queries removed: {short_removed:,}\")\n",
    "    else:\n",
    "        print(f\"    No queries shorter than 10 characters\")\n",
    "\n",
    "# Cleaning Operation 5: Handle edge cases\n",
    "print(f\"\\n Cleaning edge cases...\")\n",
    "before_edge = len(df)\n",
    "\n",
    "# Remove queries that are only whitespace after stripping\n",
    "if 'query' in df.columns:\n",
    "    df = df[df['query'].astype(str).str.strip().str.len() > 0].copy()\n",
    "    \n",
    "    # Standardize whitespace in queries\n",
    "    df['query'] = df['query'].astype(str).str.strip()\n",
    "    df['query'] = df['query'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "edge_removed = before_edge - len(df)\n",
    "if edge_removed > 0:\n",
    "    print(f\"    Cleaned {edge_removed:,} edge cases (whitespace-only queries)\")\n",
    "    cleaning_log.append(f\"Edge cases cleaned: {edge_removed:,}\")\n",
    "else:\n",
    "    print(f\"    No edge cases found\")\n",
    "\n",
    "# Final cleaning summary\n",
    "final_count = len(df)\n",
    "total_removed = original_count - final_count\n",
    "retention_rate = (final_count / original_count) * 100\n",
    "\n",
    "print(f\"\\n CLEANING SUMMARY:\")\n",
    "\n",
    "print(f\"   Original dataset: {original_count:,} records\")\n",
    "print(f\"   Final dataset: {final_count:,} records\")\n",
    "print(f\"   Total removed: {total_removed:,} records ({(total_removed/original_count)*100:.1f}%)\")\n",
    "print(f\"   Data retention: {retention_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\n Cleaning log:\")\n",
    "for i, log_entry in enumerate(cleaning_log, 1):\n",
    "    print(f\"   {i}. {log_entry}\")\n",
    "\n",
    "# Check class balance after cleaning\n",
    "if 'label' in df.columns:\n",
    "    print(f\"\\n Class Distribution (After Cleaning):\")\n",
    "    class_counts_after = df['label'].value_counts().sort_index()\n",
    "    \n",
    "    for label, count in class_counts_after.items():\n",
    "        label_name = \"Normal\" if label == 0 else \"Malicious\"\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   • {label} ({label_name}): {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    balance_ratio_after = class_counts_after.max() / class_counts_after.min()\n",
    "    print(f\"   Balance ratio after cleaning: {balance_ratio_after:.2f}:1\")\n",
    "    \n",
    "    if balance_ratio_after < 1.5:\n",
    "        print(f\"    Excellent class balance maintained!\")\n",
    "    elif balance_ratio_after < 2.0:\n",
    "        print(f\"    Good class balance maintained\")\n",
    "    else:\n",
    "        print(f\"   Class balance affected by cleaning\")\n",
    "\n",
    "df_cleaned = df.copy()\n",
    "print(f\"\\n Data cleaning completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6854cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98878f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174b72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24344f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f02317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84262f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120842c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596115e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eafce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956d81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb9665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dc112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
