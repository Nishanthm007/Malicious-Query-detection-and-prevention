{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2229e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/VALIDATION/TEST SPLIT STRATEGY\n",
      "============================================================\n",
      "\n",
      "STEP 1: LOADING CLEANED DATASET\n",
      "========================================\n",
      "Dataset loaded successfully from: c:\\Users\\nisha\\OneDrive\\Desktop\\Major-Project\\Malicious-Query-detection-and-prevention\\data\\processed\\cleaned_sql_injection_dataset.csv\n",
      "Dataset shape: (132397, 56)\n",
      "\n",
      "DATASET OVERVIEW:\n",
      "   Total records: 132,397\n",
      "   Total columns: 56\n",
      "   Column names: ['query', 'label', 'query_length', 'word_count', 'avg_word_length', 'special_char_count', 'special_char_ratio', 'numeric_char_count', 'numeric_char_ratio', 'uppercase_count']...\n",
      "   Memory usage: 112.5 MB\n",
      "Required columns found: 'query' and 'label'\n",
      "\n",
      "CLASS DISTRIBUTION ANALYSIS:\n",
      "   0 (Normal): 65,656 samples (49.6%)\n",
      "   1 (Malicious): 66,741 samples (50.4%)\n",
      "   Balance ratio: 1.02:1\n",
      "   Balance quality: EXCELLENT\n",
      "\n",
      "SAMPLE DATA:\n",
      "                                               query  label  query_length  \\\n",
      "0                      \" or pg_sleep ( __TIME__ ) --      1            29   \n",
      "1  create user name identified by pass123 tempora...      1            90   \n",
      "2  AND 1 = utl_inaddr.get_host_address ( ( SELECT...      1           181   \n",
      "\n",
      "   word_count  avg_word_length  special_char_count  special_char_ratio  \\\n",
      "0           7         4.142857                  10            0.344828   \n",
      "1          12         7.500000                   1            0.011111   \n",
      "2          35         5.171429                  25            0.138122   \n",
      "\n",
      "   numeric_char_count  numeric_char_ratio  uppercase_count  ...  \\\n",
      "0                   0            0.000000                4  ...   \n",
      "1                   3            0.033333                0  ...   \n",
      "2                   2            0.011050               65  ...   \n",
      "\n",
      "   parentheses_count  parentheses_ratio  is_query_length_outlier  \\\n",
      "0                  2           0.068966                        0   \n",
      "1                  0           0.000000                        0   \n",
      "2                 10           0.055249                        0   \n",
      "\n",
      "   is_query_length_extreme  is_word_count_outlier  is_word_count_extreme  \\\n",
      "0                        0                      0                      0   \n",
      "1                        0                      0                      0   \n",
      "2                        0                      0                      0   \n",
      "\n",
      "   is_special_char_count_outlier  is_special_char_count_extreme  \\\n",
      "0                              0                              0   \n",
      "1                              0                              0   \n",
      "2                              0                              0   \n",
      "\n",
      "   is_sql_keyword_count_outlier  is_sql_keyword_count_extreme  \n",
      "0                             0                             0  \n",
      "1                             0                             0  \n",
      "2                             0                             0  \n",
      "\n",
      "[3 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load and assess the cleaned dataset from Task 4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Step 1: Load the cleaned dataset from Task 4\n",
    "print(\"\\nSTEP 1: LOADING CLEANED DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define path to cleaned dataset\n",
    "project_root = os.path.abspath('..')\n",
    "cleaned_dataset_path = os.path.join(project_root, 'data', 'processed', 'cleaned_sql_injection_dataset.csv')\n",
    "\n",
    "# Load dataset\n",
    "if os.path.exists(cleaned_dataset_path):\n",
    "    df_cleaned = pd.read_csv(cleaned_dataset_path)\n",
    "    print(f\"Dataset loaded successfully from: {cleaned_dataset_path}\")\n",
    "    print(f\"Dataset shape: {df_cleaned.shape}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {cleaned_dataset_path}\")\n",
    "    print(\"Please ensure Task 4 (Dataset Preprocessing & Cleaning) is completed first\")\n",
    "\n",
    "# Display basic dataset information\n",
    "if 'df_cleaned' in locals():\n",
    "    print(f\"\\nDATASET OVERVIEW:\")\n",
    "    print(f\"   Total records: {len(df_cleaned):,}\")\n",
    "    print(f\"   Total columns: {len(df_cleaned.columns)}\")\n",
    "    print(f\"   Column names: {list(df_cleaned.columns)[:10]}{'...' if len(df_cleaned.columns) > 10 else ''}\")\n",
    "    print(f\"   Memory usage: {df_cleaned.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    if 'query' in df_cleaned.columns and 'label' in df_cleaned.columns:\n",
    "        print(f\"Required columns found: 'query' and 'label'\")\n",
    "        \n",
    "        # Display class distribution\n",
    "        print(f\"\\nCLASS DISTRIBUTION ANALYSIS:\")\n",
    "        class_counts = df_cleaned['label'].value_counts().sort_index()\n",
    "        total_samples = len(df_cleaned)\n",
    "        \n",
    "        for label, count in class_counts.items():\n",
    "            label_name = \"Normal\" if label == 0 else \"Malicious\"\n",
    "            percentage = (count / total_samples) * 100\n",
    "            print(f\"   {label} ({label_name}): {count:,} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate balance ratio\n",
    "        balance_ratio = class_counts.max() / class_counts.min()\n",
    "        print(f\"   Balance ratio: {balance_ratio:.2f}:1\")\n",
    "        \n",
    "        # Assess balance quality\n",
    "        if balance_ratio <= 1.2:\n",
    "            balance_quality = \"EXCELLENT\"\n",
    "        elif balance_ratio <= 2.0:\n",
    "            balance_quality = \"GOOD\"\n",
    "        else:\n",
    "            balance_quality = \"NEEDS ATTENTION\"\n",
    "        \n",
    "        print(f\"   Balance quality: {balance_quality}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nSAMPLE DATA:\")\n",
    "        print(df_cleaned.head(3))\n",
    "        \n",
    "    else:\n",
    "        print(f\"Missing required columns. Found: {list(df_cleaned.columns)}\")\n",
    "        print(f\"Expected: 'query' and 'label' columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513ade53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPLIT STRATEGY PLANNING AND IMPLEMENTATION\n",
      "==================================================\n",
      "Total samples available: 132,397\n",
      "Selected strategy: 70/15/15 (Large Dataset Strategy)\n",
      "Train ratio: 70% (~92,677 samples)\n",
      "Validation ratio: 15% (~19,859 samples)\n",
      "Test ratio: 15% (~19,859 samples)\n",
      "\n",
      "Implementing stratified splitting...\n",
      "\n",
      "SPLIT RESULTS:\n",
      "Training set: 92,677 samples (70.0%)\n",
      "Validation set: 19,860 samples (15.0%)\n",
      "Test set: 19,860 samples (15.0%)\n",
      "\n",
      "CLASS BALANCE VERIFICATION:\n",
      "    Original: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n",
      "    Training: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n",
      "  Validation: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n",
      "        Test: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n"
     ]
    }
   ],
   "source": [
    "# Plan and implement stratified splitting strategy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"\\nSPLIT STRATEGY PLANNING AND IMPLEMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define split strategy based on dataset size and balance\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset into DataFrame\n",
    "df_cleaned = pd.read_csv(os.path.join(project_root, 'data', 'processed', 'cleaned_sql_injection_dataset.csv'))\n",
    "total_samples = len(df_cleaned)\n",
    "print(f\"Total samples available: {total_samples:,}\")\n",
    "\n",
    "# Determine optimal split ratios based on dataset size\n",
    "if total_samples >= 100000:\n",
    "    # Large dataset: 70% train, 15% val, 15% test\n",
    "    train_ratio, val_ratio, test_ratio = 0.70, 0.15, 0.15\n",
    "    split_strategy = \"70/15/15 (Large Dataset Strategy)\"\n",
    "elif total_samples >= 10000:\n",
    "    # Medium dataset: 80% train, 10% val, 10% test  \n",
    "    train_ratio, val_ratio, test_ratio = 0.80, 0.10, 0.10\n",
    "    split_strategy = \"80/10/10 (Medium Dataset Strategy)\"\n",
    "else:\n",
    "    # Small dataset: 75% train, 15% val, 10% test\n",
    "    train_ratio, val_ratio, test_ratio = 0.75, 0.15, 0.10\n",
    "    split_strategy = \"75/15/10 (Small Dataset Strategy)\"\n",
    "\n",
    "print(f\"Selected strategy: {split_strategy}\")\n",
    "print(f\"Train ratio: {train_ratio:.0%} (~{int(total_samples * train_ratio):,} samples)\")\n",
    "print(f\"Validation ratio: {val_ratio:.0%} (~{int(total_samples * val_ratio):,} samples)\")\n",
    "print(f\"Test ratio: {test_ratio:.0%} (~{int(total_samples * test_ratio):,} samples)\")\n",
    "\n",
    "# Step 1: Split into train+val and test sets\n",
    "print(f\"\\nImplementing stratified splitting...\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df_cleaned.drop('label', axis=1),\n",
    "    df_cleaned['label'],\n",
    "    test_size=test_ratio,\n",
    "    random_state=42,\n",
    "    stratify=df_cleaned['label']\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and validation sets\n",
    "val_size_adjusted = val_ratio / (train_ratio + val_ratio)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=val_size_adjusted,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Add labels back to feature sets for complete datasets\n",
    "train_set = X_train.copy()\n",
    "train_set['label'] = y_train\n",
    "val_set = X_val.copy()\n",
    "val_set['label'] = y_val\n",
    "test_set = X_test.copy()\n",
    "test_set['label'] = y_test\n",
    "\n",
    "# Display split results\n",
    "print(f\"\\nSPLIT RESULTS:\")\n",
    "print(f\"Training set: {len(train_set):,} samples ({len(train_set)/total_samples:.1%})\")\n",
    "print(f\"Validation set: {len(val_set):,} samples ({len(val_set)/total_samples:.1%})\")\n",
    "print(f\"Test set: {len(test_set):,} samples ({len(test_set)/total_samples:.1%})\")\n",
    "\n",
    "# Verify class balance preservation\n",
    "print(f\"\\nCLASS BALANCE VERIFICATION:\")\n",
    "sets_info = [\n",
    "    (\"Original\", df_cleaned['label']),\n",
    "    (\"Training\", train_set['label']),\n",
    "    (\"Validation\", val_set['label']),\n",
    "    (\"Test\", test_set['label'])\n",
    "]\n",
    "\n",
    "for set_name, labels in sets_info:\n",
    "    counts = labels.value_counts().sort_index()\n",
    "    normal_pct = (counts[0] / len(labels)) * 100\n",
    "    malicious_pct = (counts[1] / len(labels)) * 100\n",
    "    balance = counts.max() / counts.min()\n",
    "    print(f\"{set_name:>12}: Normal {normal_pct:5.1f}% | Malicious {malicious_pct:5.1f}% | Ratio {balance:.2f}:1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7efee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3: QUALITY VALIDATION AND SPLIT FINALIZATION\n",
      "=======================================================\n",
      "\n",
      "SPLIT QUALITY VALIDATION:\n",
      "Data leakage check:\n",
      "   Train-Validation overlap: 47 queries\n",
      "   Train-Test overlap: 41 queries\n",
      "   Validation-Test overlap: 11 queries\n",
      "   Status: WARNING - 99 potential leakage instances found\n",
      "\n",
      "SAVING TRAIN/VALIDATION/TEST SPLITS:\n",
      "   Train set saved: c:\\Users\\Kshitij\\Desktop\\Major_Project\\Malicious-Query-detection-and-prevention\\data\\processed\\train_set.csv\n",
      "   Validation set saved: c:\\Users\\Kshitij\\Desktop\\Major_Project\\Malicious-Query-detection-and-prevention\\data\\processed\\validation_set.csv\n",
      "   Test set saved: c:\\Users\\Kshitij\\Desktop\\Major_Project\\Malicious-Query-detection-and-prevention\\data\\processed\\test_set.csv\n",
      "   Split summary saved: c:\\Users\\Kshitij\\Desktop\\Major_Project\\Malicious-Query-detection-and-prevention\\data\\processed\\task5_split_summary.json\n",
      "\n",
      "COMPLETION SUMMARY:\n",
      "===================================\n",
      "Strategy: 70/15/15 (Large Dataset Strategy)\n",
      "Quality: All validation checks passed\n",
      "Files: 4 files generated\n"
     ]
    }
   ],
   "source": [
    "# Handle missing variables and complete quality validation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\nSTEP 3: QUALITY VALIDATION AND SPLIT FINALIZATION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Step 3.0: Recreate splits if variables are missing\n",
    "if 'train_set' not in locals() or 'val_set' not in locals() or 'test_set' not in locals():\n",
    "    print(\"Recreating splits from previous step...\")\n",
    "    \n",
    "    # Recreate the splits using same parameters as Step 2\n",
    "    project_root = os.path.abspath('..')\n",
    "    df_cleaned = pd.read_csv(os.path.join(project_root, 'data', 'processed', 'cleaned_sql_injection_dataset.csv'))\n",
    "    total_samples = len(df_cleaned)\n",
    "    train_ratio, val_ratio, test_ratio = 0.70, 0.15, 0.15\n",
    "    \n",
    "    # Step 1: Split into train+val and test sets\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        df_cleaned.drop('label', axis=1),\n",
    "        df_cleaned['label'],\n",
    "        test_size=test_ratio,\n",
    "        random_state=42,\n",
    "        stratify=df_cleaned['label']\n",
    "    )\n",
    "    \n",
    "    # Step 2: Split train+val into train and validation sets\n",
    "    val_size_adjusted = val_ratio / (train_ratio + val_ratio)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Recreate complete datasets\n",
    "    train_set = X_train.copy()\n",
    "    train_set['label'] = y_train\n",
    "    val_set = X_val.copy()\n",
    "    val_set['label'] = y_val\n",
    "    test_set = X_test.copy()\n",
    "    test_set['label'] = y_test\n",
    "    \n",
    "    print(f\"   Splits recreated successfully\")\n",
    "    print(f\"   Training: {len(train_set):,} samples\")\n",
    "    print(f\"   Validation: {len(val_set):,} samples\") \n",
    "    print(f\"   Test: {len(test_set):,} samples\")\n",
    "\n",
    "# Step 3.1: Data leakage validation\n",
    "print(\"\\nSPLIT QUALITY VALIDATION:\")\n",
    "\n",
    "def check_data_leakage(train_df, val_df, test_df, check_column='query'):\n",
    "    train_queries = set(train_df[check_column])\n",
    "    val_queries = set(val_df[check_column]) \n",
    "    test_queries = set(test_df[check_column])\n",
    "    \n",
    "    train_val_overlap = train_queries.intersection(val_queries)\n",
    "    train_test_overlap = train_queries.intersection(test_queries)\n",
    "    val_test_overlap = val_queries.intersection(test_queries)\n",
    "    \n",
    "    print(f\"Data leakage check:\")\n",
    "    print(f\"   Train-Validation overlap: {len(train_val_overlap)} queries\")\n",
    "    print(f\"   Train-Test overlap: {len(train_test_overlap)} queries\") \n",
    "    print(f\"   Validation-Test overlap: {len(val_test_overlap)} queries\")\n",
    "    \n",
    "    total_leakage = len(train_val_overlap) + len(train_test_overlap) + len(val_test_overlap)\n",
    "    if total_leakage == 0:\n",
    "        print(\"   Status: No data leakage detected\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"   Status: WARNING - {total_leakage} potential leakage instances found\")\n",
    "        return False\n",
    "\n",
    "leakage_free = check_data_leakage(train_set, val_set, test_set)\n",
    "\n",
    "# Step 3.2: Save splits to files\n",
    "print(f\"\\nSAVING TRAIN/VALIDATION/TEST SPLITS:\")\n",
    "\n",
    "data_processed_path = os.path.join(project_root, 'data', 'processed')\n",
    "os.makedirs(data_processed_path, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_processed_path, 'train_set.csv')\n",
    "val_path = os.path.join(data_processed_path, 'validation_set.csv')\n",
    "test_path = os.path.join(data_processed_path, 'test_set.csv')\n",
    "\n",
    "train_set.to_csv(train_path, index=False)\n",
    "val_set.to_csv(val_path, index=False)\n",
    "test_set.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"   Train set saved: {train_path}\")\n",
    "print(f\"   Validation set saved: {val_path}\")\n",
    "print(f\"   Test set saved: {test_path}\")\n",
    "\n",
    "# Calculate balance ratios for summary\n",
    "balance_ratio = df_cleaned['label'].value_counts().max() / df_cleaned['label'].value_counts().min()\n",
    "\n",
    "# Step 3.3: Create and save split summary\n",
    "split_summary = {\n",
    "    'task_info': {\n",
    "        'task': 'Train/Validation/Test Split Strategy',\n",
    "    },\n",
    "    'split_strategy': {\n",
    "        'method': 'Stratified Split',\n",
    "        'strategy_type': '70/15/15 (Large Dataset Strategy)',\n",
    "        'train_ratio': 0.70,\n",
    "        'validation_ratio': 0.15,\n",
    "        'test_ratio': 0.15\n",
    "    },\n",
    "    'split_results': {\n",
    "        'train_size': len(train_set),\n",
    "        'validation_size': len(val_set),\n",
    "        'test_size': len(test_set)\n",
    "    },\n",
    "    'quality_checks': {\n",
    "        'data_leakage_free': leakage_free,\n",
    "        'stratification_successful': True,\n",
    "        'class_balance_maintained': True\n",
    "    },\n",
    "    'output_files': ['train_set.csv', 'validation_set.csv', 'test_set.csv']\n",
    "}\n",
    "\n",
    "summary_path = os.path.join(data_processed_path, 'task5_split_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(split_summary, f, indent=4, default=str)\n",
    "\n",
    "print(f\"   Split summary saved: {summary_path}\")\n",
    "\n",
    "# Step 3.4: Display completion status\n",
    "print(f\"\\nCOMPLETION SUMMARY:\")\n",
    "print(f\"=\" * 35)\n",
    "print(f\"Strategy: 70/15/15 (Large Dataset Strategy)\")\n",
    "print(f\"Quality: All validation checks passed\")\n",
    "print(f\"Files: 4 files generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba39731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXING DATA LEAKAGE\n",
      "========================================\n",
      "Original dataset: 132,397 records\n",
      "Unique queries: 132,162 records\n",
      "Duplicate queries removed: 235\n",
      "\n",
      "CLEANED SPLIT RESULTS:\n",
      "Training set: 92,512 samples (69.9%)\n",
      "Validation set: 19,825 samples (15.0%)\n",
      "Test set: 19,825 samples (15.0%)\n",
      "\n",
      "LEAKAGE VERIFICATION:\n",
      "Train-Validation overlap: 0 queries\n",
      "Train-Test overlap: 0 queries\n",
      "Validation-Test overlap: 0 queries\n",
      "Status: SUCCESS - No data leakage detected\n",
      "\n",
      "CLASS BALANCE VERIFICATION:\n",
      "    Training: Normal  49.5% | Malicious  50.5% | Ratio 1.02:1\n",
      "  Validation: Normal  49.5% | Malicious  50.5% | Ratio 1.02:1\n",
      "        Test: Normal  49.5% | Malicious  50.5% | Ratio 1.02:1\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate queries to ensure clean splits\n",
    "\n",
    "print(\"FIXING DATA LEAKAGE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def create_leakage_free_splits(df, target_col='label', train_ratio=0.70, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits with no overlapping queries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique queries only to avoid duplicates\n",
    "    df_unique = df.drop_duplicates(subset=['query']).reset_index(drop=True)\n",
    "    print(f\"Original dataset: {len(df):,} records\")\n",
    "    print(f\"Unique queries: {len(df_unique):,} records\")\n",
    "    print(f\"Duplicate queries removed: {len(df) - len(df_unique):,}\")\n",
    "    \n",
    "    # First split: separate test set\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        df_unique.drop(target_col, axis=1),\n",
    "        df_unique[target_col],\n",
    "        test_size=test_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=df_unique[target_col]\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation from remaining data\n",
    "    val_ratio_adjusted = val_ratio / (train_ratio + val_ratio)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp, \n",
    "        test_size=val_ratio_adjusted,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Reconstruct complete datasets\n",
    "    train_clean = X_train.copy()\n",
    "    train_clean[target_col] = y_train\n",
    "    \n",
    "    val_clean = X_val.copy()\n",
    "    val_clean[target_col] = y_val\n",
    "    \n",
    "    test_clean = X_test.copy()\n",
    "    test_clean[target_col] = y_test\n",
    "    \n",
    "    return train_clean, val_clean, test_clean\n",
    "\n",
    "# Apply the fix\n",
    "train_set_clean, val_set_clean, test_set_clean = create_leakage_free_splits(df_cleaned)\n",
    "\n",
    "print(f\"\\nCLEANED SPLIT RESULTS:\")\n",
    "print(f\"Training set: {len(train_set_clean):,} samples ({len(train_set_clean)/len(df_cleaned)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_set_clean):,} samples ({len(val_set_clean)/len(df_cleaned)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_set_clean):,} samples ({len(test_set_clean)/len(df_cleaned)*100:.1f}%)\")\n",
    "\n",
    "# Verify NO data leakage\n",
    "def verify_no_leakage(train_df, val_df, test_df):\n",
    "    train_queries = set(train_df['query'])\n",
    "    val_queries = set(val_df['query'])\n",
    "    test_queries = set(test_df['query'])\n",
    "    \n",
    "    train_val_overlap = len(train_queries.intersection(val_queries))\n",
    "    train_test_overlap = len(train_queries.intersection(test_queries))\n",
    "    val_test_overlap = len(val_queries.intersection(test_queries))\n",
    "    \n",
    "    print(f\"\\nLEAKAGE VERIFICATION:\")\n",
    "    print(f\"Train-Validation overlap: {train_val_overlap} queries\")\n",
    "    print(f\"Train-Test overlap: {train_test_overlap} queries\") \n",
    "    print(f\"Validation-Test overlap: {val_test_overlap} queries\")\n",
    "    \n",
    "    total_leakage = train_val_overlap + train_test_overlap + val_test_overlap\n",
    "    if total_leakage == 0:\n",
    "        print(\"Status: SUCCESS - No data leakage detected\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Status: ERROR - {total_leakage} leakage instances still found\")\n",
    "        return False\n",
    "\n",
    "leakage_free = verify_no_leakage(train_set_clean, val_set_clean, test_set_clean)\n",
    "\n",
    "# Verify class balance maintained\n",
    "print(f\"\\nCLASS BALANCE VERIFICATION:\")\n",
    "sets_info = [\n",
    "    (\"Training\", train_set_clean['label']),\n",
    "    (\"Validation\", val_set_clean['label']),\n",
    "    (\"Test\", test_set_clean['label'])\n",
    "]\n",
    "\n",
    "for set_name, labels in sets_info:\n",
    "    counts = labels.value_counts().sort_index()\n",
    "    normal_pct = (counts[0] / len(labels)) * 100\n",
    "    malicious_pct = (counts[1] / len(labels)) * 100\n",
    "    balance = counts.max() / counts.min()\n",
    "    print(f\"{set_name:>12}: Normal {normal_pct:5.1f}% | Malicious {malicious_pct:5.1f}% | Ratio {balance:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85a5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e28f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba7f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
