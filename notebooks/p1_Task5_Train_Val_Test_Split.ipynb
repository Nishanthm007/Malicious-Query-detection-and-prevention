{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2229e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/VALIDATION/TEST SPLIT STRATEGY\n",
      "============================================================\n",
      "\n",
      "STEP 1: LOADING CLEANED DATASET\n",
      "========================================\n",
      "Dataset loaded successfully from: c:\\Users\\nisha\\OneDrive\\Desktop\\Major-Project\\Malicious-Query-detection-and-prevention\\data\\processed\\cleaned_sql_injection_dataset.csv\n",
      "Dataset shape: (132397, 56)\n",
      "\n",
      "DATASET OVERVIEW:\n",
      "   Total records: 132,397\n",
      "   Total columns: 56\n",
      "   Column names: ['query', 'label', 'query_length', 'word_count', 'avg_word_length', 'special_char_count', 'special_char_ratio', 'numeric_char_count', 'numeric_char_ratio', 'uppercase_count']...\n",
      "   Memory usage: 112.5 MB\n",
      "Required columns found: 'query' and 'label'\n",
      "\n",
      "CLASS DISTRIBUTION ANALYSIS:\n",
      "   0 (Normal): 65,656 samples (49.6%)\n",
      "   1 (Malicious): 66,741 samples (50.4%)\n",
      "   Balance ratio: 1.02:1\n",
      "   Balance quality: EXCELLENT\n",
      "\n",
      "SAMPLE DATA:\n",
      "                                               query  label  query_length  \\\n",
      "0                      \" or pg_sleep ( __TIME__ ) --      1            29   \n",
      "1  create user name identified by pass123 tempora...      1            90   \n",
      "2  AND 1 = utl_inaddr.get_host_address ( ( SELECT...      1           181   \n",
      "\n",
      "   word_count  avg_word_length  special_char_count  special_char_ratio  \\\n",
      "0           7         4.142857                  10            0.344828   \n",
      "1          12         7.500000                   1            0.011111   \n",
      "2          35         5.171429                  25            0.138122   \n",
      "\n",
      "   numeric_char_count  numeric_char_ratio  uppercase_count  ...  \\\n",
      "0                   0            0.000000                4  ...   \n",
      "1                   3            0.033333                0  ...   \n",
      "2                   2            0.011050               65  ...   \n",
      "\n",
      "   parentheses_count  parentheses_ratio  is_query_length_outlier  \\\n",
      "0                  2           0.068966                        0   \n",
      "1                  0           0.000000                        0   \n",
      "2                 10           0.055249                        0   \n",
      "\n",
      "   is_query_length_extreme  is_word_count_outlier  is_word_count_extreme  \\\n",
      "0                        0                      0                      0   \n",
      "1                        0                      0                      0   \n",
      "2                        0                      0                      0   \n",
      "\n",
      "   is_special_char_count_outlier  is_special_char_count_extreme  \\\n",
      "0                              0                              0   \n",
      "1                              0                              0   \n",
      "2                              0                              0   \n",
      "\n",
      "   is_sql_keyword_count_outlier  is_sql_keyword_count_extreme  \n",
      "0                             0                             0  \n",
      "1                             0                             0  \n",
      "2                             0                             0  \n",
      "\n",
      "[3 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load and assess the cleaned dataset from Task 4\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Step 1: Load the cleaned dataset from Task 4\n",
    "print(\"\\nSTEP 1: LOADING CLEANED DATASET\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define path to cleaned dataset\n",
    "project_root = os.path.abspath('..')\n",
    "cleaned_dataset_path = os.path.join(project_root, 'data', 'processed', 'cleaned_sql_injection_dataset.csv')\n",
    "\n",
    "# Load dataset\n",
    "if os.path.exists(cleaned_dataset_path):\n",
    "    df_cleaned = pd.read_csv(cleaned_dataset_path)\n",
    "    print(f\"Dataset loaded successfully from: {cleaned_dataset_path}\")\n",
    "    print(f\"Dataset shape: {df_cleaned.shape}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at: {cleaned_dataset_path}\")\n",
    "    print(\"Please ensure Task 4 (Dataset Preprocessing & Cleaning) is completed first\")\n",
    "\n",
    "# Display basic dataset information\n",
    "if 'df_cleaned' in locals():\n",
    "    print(f\"\\nDATASET OVERVIEW:\")\n",
    "    print(f\"   Total records: {len(df_cleaned):,}\")\n",
    "    print(f\"   Total columns: {len(df_cleaned.columns)}\")\n",
    "    print(f\"   Column names: {list(df_cleaned.columns)[:10]}{'...' if len(df_cleaned.columns) > 10 else ''}\")\n",
    "    print(f\"   Memory usage: {df_cleaned.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    if 'query' in df_cleaned.columns and 'label' in df_cleaned.columns:\n",
    "        print(f\"Required columns found: 'query' and 'label'\")\n",
    "        \n",
    "        # Display class distribution\n",
    "        print(f\"\\nCLASS DISTRIBUTION ANALYSIS:\")\n",
    "        class_counts = df_cleaned['label'].value_counts().sort_index()\n",
    "        total_samples = len(df_cleaned)\n",
    "        \n",
    "        for label, count in class_counts.items():\n",
    "            label_name = \"Normal\" if label == 0 else \"Malicious\"\n",
    "            percentage = (count / total_samples) * 100\n",
    "            print(f\"   {label} ({label_name}): {count:,} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate balance ratio\n",
    "        balance_ratio = class_counts.max() / class_counts.min()\n",
    "        print(f\"   Balance ratio: {balance_ratio:.2f}:1\")\n",
    "        \n",
    "        # Assess balance quality\n",
    "        if balance_ratio <= 1.2:\n",
    "            balance_quality = \"EXCELLENT\"\n",
    "        elif balance_ratio <= 2.0:\n",
    "            balance_quality = \"GOOD\"\n",
    "        else:\n",
    "            balance_quality = \"NEEDS ATTENTION\"\n",
    "        \n",
    "        print(f\"   Balance quality: {balance_quality}\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(f\"\\nSAMPLE DATA:\")\n",
    "        print(df_cleaned.head(3))\n",
    "        \n",
    "    else:\n",
    "        print(f\"Missing required columns. Found: {list(df_cleaned.columns)}\")\n",
    "        print(f\"Expected: 'query' and 'label' columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513ade53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPLIT STRATEGY PLANNING AND IMPLEMENTATION\n",
      "==================================================\n",
      "Total samples available: 132,397\n",
      "Selected strategy: 70/15/15 (Large Dataset Strategy)\n",
      "Train ratio: 70% (~92,677 samples)\n",
      "Validation ratio: 15% (~19,859 samples)\n",
      "Test ratio: 15% (~19,859 samples)\n",
      "\n",
      "Implementing stratified splitting...\n",
      "\n",
      "SPLIT RESULTS:\n",
      "Training set: 92,677 samples (70.0%)\n",
      "Validation set: 19,860 samples (15.0%)\n",
      "Test set: 19,860 samples (15.0%)\n",
      "\n",
      "CLASS BALANCE VERIFICATION:\n",
      "    Original: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n",
      "    Training: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n",
      "  Validation: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n",
      "        Test: Normal  49.6% | Malicious  50.4% | Ratio 1.02:1\n"
     ]
    }
   ],
   "source": [
    "# Plan and implement stratified splitting strategy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"\\nSPLIT STRATEGY PLANNING AND IMPLEMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define split strategy based on dataset size and balance\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset into DataFrame\n",
    "df_cleaned = pd.read_csv(os.path.join(project_root, 'data', 'processed', 'cleaned_sql_injection_dataset.csv'))\n",
    "total_samples = len(df_cleaned)\n",
    "print(f\"Total samples available: {total_samples:,}\")\n",
    "\n",
    "# Determine optimal split ratios based on dataset size\n",
    "if total_samples >= 100000:\n",
    "    # Large dataset: 70% train, 15% val, 15% test\n",
    "    train_ratio, val_ratio, test_ratio = 0.70, 0.15, 0.15\n",
    "    split_strategy = \"70/15/15 (Large Dataset Strategy)\"\n",
    "elif total_samples >= 10000:\n",
    "    # Medium dataset: 80% train, 10% val, 10% test  \n",
    "    train_ratio, val_ratio, test_ratio = 0.80, 0.10, 0.10\n",
    "    split_strategy = \"80/10/10 (Medium Dataset Strategy)\"\n",
    "else:\n",
    "    # Small dataset: 75% train, 15% val, 10% test\n",
    "    train_ratio, val_ratio, test_ratio = 0.75, 0.15, 0.10\n",
    "    split_strategy = \"75/15/10 (Small Dataset Strategy)\"\n",
    "\n",
    "print(f\"Selected strategy: {split_strategy}\")\n",
    "print(f\"Train ratio: {train_ratio:.0%} (~{int(total_samples * train_ratio):,} samples)\")\n",
    "print(f\"Validation ratio: {val_ratio:.0%} (~{int(total_samples * val_ratio):,} samples)\")\n",
    "print(f\"Test ratio: {test_ratio:.0%} (~{int(total_samples * test_ratio):,} samples)\")\n",
    "\n",
    "# Step 1: Split into train+val and test sets\n",
    "print(f\"\\nImplementing stratified splitting...\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df_cleaned.drop('label', axis=1),\n",
    "    df_cleaned['label'],\n",
    "    test_size=test_ratio,\n",
    "    random_state=42,\n",
    "    stratify=df_cleaned['label']\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and validation sets\n",
    "val_size_adjusted = val_ratio / (train_ratio + val_ratio)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=val_size_adjusted,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Add labels back to feature sets for complete datasets\n",
    "train_set = X_train.copy()\n",
    "train_set['label'] = y_train\n",
    "val_set = X_val.copy()\n",
    "val_set['label'] = y_val\n",
    "test_set = X_test.copy()\n",
    "test_set['label'] = y_test\n",
    "\n",
    "# Display split results\n",
    "print(f\"\\nSPLIT RESULTS:\")\n",
    "print(f\"Training set: {len(train_set):,} samples ({len(train_set)/total_samples:.1%})\")\n",
    "print(f\"Validation set: {len(val_set):,} samples ({len(val_set)/total_samples:.1%})\")\n",
    "print(f\"Test set: {len(test_set):,} samples ({len(test_set)/total_samples:.1%})\")\n",
    "\n",
    "# Verify class balance preservation\n",
    "print(f\"\\nCLASS BALANCE VERIFICATION:\")\n",
    "sets_info = [\n",
    "    (\"Original\", df_cleaned['label']),\n",
    "    (\"Training\", train_set['label']),\n",
    "    (\"Validation\", val_set['label']),\n",
    "    (\"Test\", test_set['label'])\n",
    "]\n",
    "\n",
    "for set_name, labels in sets_info:\n",
    "    counts = labels.value_counts().sort_index()\n",
    "    normal_pct = (counts[0] / len(labels)) * 100\n",
    "    malicious_pct = (counts[1] / len(labels)) * 100\n",
    "    balance = counts.max() / counts.min()\n",
    "    print(f\"{set_name:>12}: Normal {normal_pct:5.1f}% | Malicious {malicious_pct:5.1f}% | Ratio {balance:.2f}:1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7efee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba39731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85a5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e28f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba7f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
